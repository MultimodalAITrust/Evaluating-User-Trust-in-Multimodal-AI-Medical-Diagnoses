# Evaluating-User-Trust-in-Multimodal-AI-Medical-Diagnoses

## ğŸ“Œ Project Overview
Large multimodal language models (MLLMs) such as **LLaVA** and **BLIP-2** have unlocked new possibilities for **AI-assisted medical diagnosis** by bridging vision and language. However, in medical contexts, **trust** goes beyond accuracy â€” it depends on:

- âœ… Clarity of explanations  
- âœ… Calibrated uncertainty  
- âœ… Resource-efficient deployment

We conducted a **two-phase study** to evaluate user trust in **general-purpose** vs. **medically trained** LLMs for diagnostic tasks.

---

## ğŸ§ª Study Phases

### **Phase 1: Formal User Study**
- **Models Tested:** Gemini 1.5 Pro *(general)* vs. BioGPT *(medical, BLIP-2 vision grounding)*
- **Finding:** Users preferred **Gemini** due to **clearer, concise explanations** despite weaker domain expertise.

### **Phase 2: Exploratory Trial (Ongoing)**
- **Models Tested:** Qwen3 *(general)* vs. Qwen3-Medical *(medical)*
- **Finding:** Same pattern observed â€” **general models are clearer**, but medical ones are **more accurate** yet **verbose**.
- Neither handles images natively â†’ **BLIP preprocessing** introduced latency.

---

## âš ï¸ Key Challenges Identified
- âŒ Domain-specific models: better accuracy but **poor explanation accessibility**  
- âŒ Lack of **native multimodal integration**  
- âŒ High **compute & memory requirements** (LLaVA & LLaVA-Med)  
- âŒ Limited expertise in **medical vision encoders**  
